{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e345a-d608-4141-b39c-28448fb6bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preparation Pipeline\n",
    "-------------------------\n",
    "This script reads product planning data, cityâ€“province mappings, \n",
    "and invoice records from SQL Server, merges them, converts Jalali \n",
    "dates to Gregorian, and performs necessary data cleaning.\n",
    "\n",
    "Sensitive information has been removed for public sharing.\n",
    "\"\"\"\n",
    "\n",
    "# ðŸ“¦ Libraries\n",
    "import pandas as pd\n",
    "import jdatetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ File paths (sanitized)\n",
    "# =============================\n",
    "file_path = r\"<PATH_TO_PRODUCT_FILE>/DataBase_Product.xlsx\"\n",
    "file_path_1 = r\"<PATH_TO_CITY_FILE>/Cities.xlsx\"\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ Read Excel files\n",
    "# =============================\n",
    "planning_df = pd.read_excel(file_path, sheet_name=\"Planning\")\n",
    "\n",
    "city_province_df = pd.read_excel(file_path_1)\n",
    "city_province_df.columns = ['CityName', 'ProvinceName']  # Normalize column names\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ SQL Server connection (sanitized)\n",
    "# =============================\n",
    "connection_string = (\n",
    "    \"mssql+pyodbc://<USERNAME>:<PASSWORD>@<SERVER>/<DATABASE>\"\n",
    "    \"?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    ")\n",
    "\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ Read tables from SQL Server\n",
    "# =============================\n",
    "query_invoice = \"SELECT * FROM dbo.HybridSale\"\n",
    "query_date = \"SELECT * FROM dbo.DimDate\"\n",
    "\n",
    "data = pd.read_sql(query_invoice, engine)\n",
    "df = pd.read_sql(query_date, engine)\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ Jalali â†’ Gregorian converter\n",
    "# =============================\n",
    "def jalali_to_gregorian(date_str):\n",
    "    \"\"\"Convert 'YYYY/MM/DD' Jalali date to Gregorian (datetime).\"\"\"\n",
    "    try:\n",
    "        y, m, d = map(int, str(date_str).split('/'))\n",
    "        g = jdatetime.date(y, m, d).togregorian()\n",
    "        return pd.to_datetime(g)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ Prepare planning data\n",
    "# =============================\n",
    "m = planning_df.iloc[:, [0, 4, 10]].copy()\n",
    "m.columns = ['merge_key', 'BrandName', 'category']\n",
    "\n",
    "data['ProductCode'] = data['ProductCode'].astype(str)\n",
    "m['merge_key'] = m['merge_key'].astype(str)\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ Merge planning info into invoices\n",
    "# =============================\n",
    "data = data.merge(\n",
    "    m[['merge_key', 'category']],\n",
    "    left_on='ProductCode',\n",
    "    right_on='merge_key',\n",
    "    how='left'\n",
    ").drop(columns=['merge_key'])\n",
    "\n",
    "# Handle duplicate category columns if they appear\n",
    "if 'category_x' in data.columns and 'category_y' in data.columns:\n",
    "    data['category'] = data['category_x'].combine_first(data['category_y'])\n",
    "    data = data.drop(columns=['category_x', 'category_y'])\n",
    "\n",
    "# Drop rows without category\n",
    "data = data.dropna(subset=['category'])\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ Add ProvinceName using CityName\n",
    "# =============================\n",
    "data = data.merge(\n",
    "    city_province_df,\n",
    "    on='CityName',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"âœ” ProvinceName column successfully added.\")\n",
    "print(f\"âœ” Final row count: {len(data):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee7eb4-2b14-420f-990b-3bc59da4a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RFM Segmentation Pipeline\n",
    "-------------------------\n",
    "This script performs RFM calculation, outlier detection, clustering,\n",
    "segment mapping, new customer identification, and time-range-based\n",
    "segmentation for customer behavior analysis.\n",
    "\n",
    "All sensitive information, column names, and paths have been replaced\n",
    "with placeholders for safe public release on GitHub.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ“¦ Imports\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ”¹ Order clusters by the mean of the target variable\n",
    "# ---------------------------------------------------------\n",
    "def order_cluster(cluster_field_name, target_field_name, df, ascending):\n",
    "    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n",
    "    df_new = df_new.sort_values(by=target_field_name, ascending=ascending).reset_index(drop=True)\n",
    "    df_new[\"index\"] = df_new.index\n",
    "    df_final = pd.merge(df, df_new[[cluster_field_name, \"index\"]], on=cluster_field_name)\n",
    "    df_final = df_final.drop(columns=[cluster_field_name])\n",
    "    df_final = df_final.rename(columns={\"index\": cluster_field_name})\n",
    "    return df_final\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ”¹ Segment Map (RFM Score â†’ Segment Name)\n",
    "# ---------------------------------------------------------\n",
    "SEGMENT_MAP = {\n",
    "    \"Champions\": {\"444\",\"443\",\"433\",\"434\",\"343\",\"344\",\"334\"},\n",
    "    \"Loyal\": {\"432\",\"333\",\"324\",\"244\",\"243\",\"234\",\"233\",\"224\"},\n",
    "    \"Potential Loyalist\": {\n",
    "        '442','440','441','430','431','422','421','420','341','340','331','330',\n",
    "        '320','342','322','321','312','242','241','240','231','230','222','212',\n",
    "        '400'\n",
    "    },\n",
    "    \"Promising\": {\n",
    "        '414','413','412','411','410','404','403','402',\n",
    "        '314','313','302','303','304','204','203','202',\n",
    "        '301','310','311','401'\n",
    "    },\n",
    "    \"Needs Attention\": {'424','423','332','323','232','223','214','213'},\n",
    "    \"About To Sleep\": {'220','210','201','110','102','200','300'},\n",
    "    \"At Risk\": {\n",
    "        '144','143','134','133','142','141','132','131',\n",
    "        '124','123','114','113','042','041','034','032',\n",
    "        '031','024','023','022','014','013'\n",
    "    },\n",
    "    \"Critical Loss Risk\": {'044','043','033','103','104','004','003','002'},\n",
    "    \"Hibernating\": {'221','211','120','130','140','122','121','112','111',\n",
    "                    '021','012','011','101','100'},\n",
    "    \"Lost\": {'000','001','010','020','030','040'}\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ”¹ Assign segment from RFM score\n",
    "# ---------------------------------------------------------\n",
    "def assign_segment_from_score(score_str: str) -> str:\n",
    "    if pd.isna(score_str):\n",
    "        return None\n",
    "    for seg, codes in SEGMENT_MAP.items():\n",
    "        if score_str in codes:\n",
    "            return seg\n",
    "    return None\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ”¹ Remove outliers via Z-Score\n",
    "# ---------------------------------------------------------\n",
    "def remove_outliers_zscore(df, cols, threshold=3.0):\n",
    "    arr = df[cols].to_numpy(dtype=float)\n",
    "    if arr.shape[0] < 2:\n",
    "        return df.copy(), pd.DataFrame(), 0\n",
    "    Z = np.abs(zscore(arr, nan_policy=\"omit\"))\n",
    "    if len(cols) == 1:\n",
    "        Z = Z.reshape(-1, 1)\n",
    "    mask = (Z < threshold) | np.isnan(Z)\n",
    "    mask = mask.all(axis=1)\n",
    "\n",
    "    removed_df = df.loc[~mask].copy()\n",
    "    kept_df = df.loc[mask].copy()\n",
    "    removed = (~mask).sum()\n",
    "\n",
    "    return kept_df, removed_df, int(removed)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ”¹ Compute RFM for a single period\n",
    "# ---------------------------------------------------------\n",
    "def RFM_one_period(df, end_date):\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    rfm = (\n",
    "        df.groupby(\"CustomerCode\")\n",
    "        .agg(\n",
    "            # Basic fields\n",
    "            CustomerName=(\"CustomerName\", \"first\"),\n",
    "            CustomerStatus=(\"CustomerStatus\", \"first\"),\n",
    "            BranchCode=(\"BranchCode\", \"first\"),\n",
    "            BranchName=(\"BranchName\", \"first\"),\n",
    "            FactorGDate=(\"FactorGDate\", \"first\"),\n",
    "            FactorSDate=(\"FactorSDate\", \"first\"),\n",
    "            FactorNumber=(\"FactorNumber\", \"first\"),\n",
    "            SupervisorCode=(\"SupervisorCode\", \"first\"),\n",
    "            SupervisorName=(\"SupervisorName\", \"first\"),\n",
    "            SupervisorStatus=(\"SupervisorStatus\", \"first\"),\n",
    "            VisitorCode=(\"VisitorCode\", \"first\"),\n",
    "            VisitorName=(\"VisitorName\", \"first\"),\n",
    "            VisitorStatus=(\"VisitorStatus\", \"first\"),\n",
    "            VisitorRoute=(\"VisitorRoute\", \"first\"),\n",
    "            StoreCode=(\"StoreCode\", \"first\"),\n",
    "            StoreName=(\"StoreName\", \"first\"),\n",
    "            StoreStatus=(\"StoreStatus\", \"first\"),\n",
    "            Province=(\"ProvinceName\", \"first\"),\n",
    "            CityName=(\"CityName\", \"first\"),\n",
    "            CityStatus=(\"CityStatus\", \"first\"),\n",
    "            GuildCode=(\"GuildCode\", \"first\"),\n",
    "            GuildType=(\"GuildType\", \"first\"),\n",
    "            PaymentType=(\"PaymentType\", \"first\"),\n",
    "            category=(\"category\", \"first\"),\n",
    "            ProductCode=(\"ProductCode\", \"first\"),\n",
    "            ProductName=(\"ProductName\", \"first\"),\n",
    "\n",
    "            # Sales metrics\n",
    "            NetCartonQuantity=(\"NetCartonQuantity\", \"sum\"),\n",
    "            ReturnCartonQuantity=(\"ReturnCartonQuantity\", \"sum\"),\n",
    "            TotalPrice=(\"TotalPrice\", \"sum\"),\n",
    "            ReturnTotalPrice=(\"ReturnTotalPrice\", \"sum\"),\n",
    "\n",
    "            # RFM\n",
    "            Recency=(\"FactorGDate\", lambda x: (end_date - x.max()).days),\n",
    "            Frequency=(\"FactorNumber\", \"nunique\"),\n",
    "            Monetary=(\"TotalPrice\", \"sum\"),\n",
    "            Returns=(\"ReturnTotalPrice\", \"sum\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Outlier filtering\n",
    "    rfm_clean, rfm_outliers, _ = remove_outliers_zscore(\n",
    "        rfm, [\"Recency\", \"Frequency\", \"Monetary\"], threshold=3.0\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Safe KMeans wrapper\n",
    "    # ------------------------------\n",
    "    def safe_kmeans(df, col, ascending=True):\n",
    "        n_clusters = min(len(df), 5)\n",
    "        if n_clusters < 2:\n",
    "            df[f\"{col}Cluster\"] = 0\n",
    "            return df\n",
    "\n",
    "        km = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "        df[f\"{col}Cluster\"] = km.fit_predict(df[[col]])\n",
    "        df = order_cluster(f\"{col}Cluster\", col, df, ascending=ascending)\n",
    "        return df\n",
    "\n",
    "    # --- Clean data clustering ---\n",
    "    if not rfm_clean.empty:\n",
    "        rfm_clean = safe_kmeans(rfm_clean, \"Recency\", ascending=False)\n",
    "        rfm_clean = safe_kmeans(rfm_clean, \"Frequency\", ascending=True)\n",
    "\n",
    "        rfm_clean[\"NetMonetary\"] = (\n",
    "            rfm_clean[\"Monetary\"] - rfm_clean[\"Returns\"]\n",
    "        ).clip(lower=0)\n",
    "\n",
    "        rfm_clean = safe_kmeans(rfm_clean, \"NetMonetary\", ascending=True)\n",
    "        rfm_clean = rfm_clean.rename(columns={\"NetMonetaryCluster\": \"MonetaryCluster\"})\n",
    "\n",
    "        rfm_clean[\"Score\"] = (\n",
    "            rfm_clean[\"RecencyCluster\"].astype(str)\n",
    "            + rfm_clean[\"FrequencyCluster\"].astype(str)\n",
    "            + rfm_clean[\"MonetaryCluster\"].astype(str)\n",
    "        )\n",
    "\n",
    "        rfm_clean[\"Segment\"] = rfm_clean[\"Score\"].apply(assign_segment_from_score)\n",
    "        rfm_clean[\"Date\"] = end_date\n",
    "\n",
    "    # --- Outlier records processing ---\n",
    "    if not rfm_outliers.empty:\n",
    "        rfm_outliers = safe_kmeans(rfm_outliers, \"Recency\", ascending=False)\n",
    "        rfm_outliers = safe_kmeans(rfm_outliers, \"Frequency\", ascending=True)\n",
    "\n",
    "        rfm_outliers[\"NetMonetary\"] = (\n",
    "            rfm_outliers[\"Monetary\"] - rfm_outliers[\"Returns\"]\n",
    "        ).clip(lower=0)\n",
    "\n",
    "        rfm_outliers = safe_kmeans(rfm_outliers, \"NetMonetary\", ascending=True)\n",
    "        rfm_outliers = rfm_outliers.rename(columns={\"NetMonetaryCluster\": \"MonetaryCluster\"})\n",
    "\n",
    "        rfm_outliers[\"Score\"] = (\n",
    "            rfm_outliers[\"RecencyCluster\"].astype(str)\n",
    "            + rfm_outliers[\"FrequencyCluster\"].astype(str)\n",
    "            + rfm_outliers[\"MonetaryCluster\"].astype(str)\n",
    "        )\n",
    "        rfm_outliers[\"Segment\"] = rfm_outliers[\"Score\"].apply(assign_segment_from_score)\n",
    "        rfm_outliers[\"Segment\"] = rfm_outliers[\"Segment\"].fillna(\"Outlier\")\n",
    "        rfm_outliers[\"Date\"] = end_date\n",
    "\n",
    "    return rfm_clean, rfm_outliers\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ”¹ Process RFM over custom ranges\n",
    "# ---------------------------------------------------------\n",
    "def RFM_over_custom_ranges(data, ranges):\n",
    "    d = data.copy()\n",
    "    d[\"FactorGDate\"] = pd.to_datetime(d[\"FactorGDate\"], errors=\"coerce\")\n",
    "\n",
    "    results, outliers = [], []\n",
    "    for start_date, end_date in ranges:\n",
    "        df_period = d[(d[\"FactorGDate\"] >= start_date) & (d[\"FactorGDate\"] <= end_date)]\n",
    "        if not df_period.empty:\n",
    "            rfm_period, rfm_outliers = RFM_one_period(df_period, end_date)\n",
    "            if not rfm_period.empty:\n",
    "                results.append(rfm_period)\n",
    "            if not rfm_outliers.empty:\n",
    "                outliers.append(rfm_outliers)\n",
    "\n",
    "    final = pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "    final_outliers = pd.concat(outliers, ignore_index=True) if outliers else pd.DataFrame()\n",
    "\n",
    "    return final, final_outliers\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ”¹ Stabilize segment transitions across time\n",
    "# ---------------------------------------------------------\n",
    "def stabilize_segments(rfm_all):\n",
    "    rfm_all = rfm_all.sort_values(by=[\"CustomerCode\", \"Date\"])\n",
    "    rfm_all[\"FinalSegment\"] = None\n",
    "\n",
    "    allowed_transitions = {\n",
    "        \"Potential Loyalist\": {\"Loyal\",\"Champions\",\"Promising\",\"Needs Attention\"},\n",
    "        \"Loyal\": {\"Champions\",\"Needs Attention\",\"At Risk\"},\n",
    "        \"Champions\": {\"Loyal\",\"Needs Attention\"},\n",
    "        \"Promising\": {\"Potential Loyalist\",\"Loyal\"},\n",
    "        \"Needs Attention\": {\"At Risk\",\"Hibernating\"},\n",
    "        \"At Risk\": {\"Critical Loss Risk\",\"Hibernating\"},\n",
    "        \"Hibernating\": {\"Lost\"},\n",
    "        \"Critical Loss Risk\": {\"Lost\"},\n",
    "    }\n",
    "\n",
    "    last_segment = {}\n",
    "\n",
    "    for idx, row in rfm_all.iterrows():\n",
    "        cust = row[\"CustomerCode\"]\n",
    "        seg = row[\"Segment\"]\n",
    "\n",
    "        if pd.isna(seg):\n",
    "            rfm_all.at[idx, \"FinalSegment\"] = \"Unknown\"\n",
    "            continue\n",
    "\n",
    "        if cust not in last_segment:\n",
    "            rfm_all.at[idx, \"FinalSegment\"] = seg\n",
    "            last_segment[cust] = seg\n",
    "        else:\n",
    "            prev_seg = last_segment[cust]\n",
    "            if seg == prev_seg or seg in allowed_transitions.get(prev_seg, set()):\n",
    "                rfm_all.at[idx, \"FinalSegment\"] = seg\n",
    "                last_segment[cust] = seg\n",
    "            else:\n",
    "                rfm_all.at[idx, \"FinalSegment\"] = prev_seg\n",
    "\n",
    "    rfm_all[\"FinalSegment\"] = rfm_all[\"FinalSegment\"].fillna(\"Unknown\")\n",
    "    return rfm_all\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ”¹ Mark newly acquired customers\n",
    "# ---------------------------------------------------------\n",
    "def mark_new_customers(rfm_all):\n",
    "    rfm_all[\"Date\"] = pd.to_datetime(rfm_all[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    first_purchases = (\n",
    "        rfm_all.groupby([\"category\", \"CustomerCode\"])[\"Date\"]\n",
    "        .min()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"Date\": \"FirstPurchaseDate\"})\n",
    "    )\n",
    "\n",
    "    rfm_all = rfm_all.merge(first_purchases, on=[\"category\", \"CustomerCode\"], how=\"left\")\n",
    "\n",
    "    rfm_all[\"Segment\"] = np.where(\n",
    "        rfm_all[\"Date\"].dt.year == rfm_all[\"FirstPurchaseDate\"].dt.year,\n",
    "        \"New Customer\",\n",
    "        rfm_all[\"Segment\"],\n",
    "    )\n",
    "\n",
    "    return rfm_all.drop(columns=[\"FirstPurchaseDate\"])\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸ”¹ Start-date filtering\n",
    "# ----------------------------\n",
    "start_filter_fixed = datetime(2024, 3, 20)\n",
    "df[\"MiladiDate\"] = pd.to_datetime(df[\"MiladiDate\"])\n",
    "df = df[df[\"MiladiDate\"] >= start_filter_fixed]\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸ”¹ Build date ranges per YearMonth\n",
    "# ----------------------------\n",
    "ranges = (\n",
    "    df.groupby(\"YearMonthName\")[\"MiladiDate\"]\n",
    "    .agg([\"min\", \"max\"])\n",
    "    .reset_index()\n",
    "    .sort_values(\"min\")\n",
    ")\n",
    "\n",
    "custom_ranges = [(row[\"min\"], row[\"max\"]) for _, row in ranges.iterrows()]\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸ”¹ Final Execution\n",
    "# ----------------------------\n",
    "output_file = \"RFM_custom_ranges.xlsx\"\n",
    "all_results, all_outliers = [], []\n",
    "\n",
    "for val in data[\"category\"].dropna().unique():\n",
    "    subset = data[data[\"category\"] == val]\n",
    "\n",
    "    rfm_result, outliers_result = RFM_over_custom_ranges(subset, custom_ranges)\n",
    "    rfm_result = stabilize_segments(rfm_result)\n",
    "    rfm_result = mark_new_customers(rfm_result)\n",
    "\n",
    "    if not rfm_result.empty:\n",
    "        rfm_result[\"Date\"] = pd.to_datetime(rfm_result[\"Date\"]).dt.date\n",
    "\n",
    "    all_results.append(rfm_result)\n",
    "    if not outliers_result.empty:\n",
    "        all_outliers.append(outliers_result)\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸ”¹ Merge Normal + Outliers\n",
    "# ----------------------------\n",
    "final_output = (\n",
    "    pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()\n",
    ")\n",
    "\n",
    "final_outliers = (\n",
    "    pd.concat(all_outliers, ignore_index=True) if all_outliers else pd.DataFrame()\n",
    ")\n",
    "\n",
    "if not final_outliers.empty:\n",
    "    final_outliers[\"OutlierFlag\"] = \"Outlier\"\n",
    "    final_output[\"OutlierFlag\"] = \"Normal\"\n",
    "    final_output = pd.concat([final_output, final_outliers], ignore_index=True)\n",
    "else:\n",
    "    final_output[\"OutlierFlag\"] = \"Normal\"\n",
    "\n",
    "# Drop previous FinalSegment if exists\n",
    "if \"FinalSegment\" in final_output.columns:\n",
    "    final_output = final_output.drop(columns=[\"FinalSegment\"])\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸ”¹ Export to Excel\n",
    "# ----------------------------\n",
    "with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "    final_output.to_excel(writer, sheet_name=\"RFM_Data\", index=False)\n",
    "\n",
    "print(f\"âœ” Output saved to '{output_file}' in sheet 'RFM_Data'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
